
<p>
  With the development of intelligent technology, robots are gradually expanding from the traditional industrial field to a wider range of application scenarios, where it is becoming increasingly important to enable robots to perform accurate navigation in complex unstructured and dynamically-changing environments. Recently, Deep Learning (DL) has greatly enhanced the perception ability of the robotic system, and Deep Reinforcement Learning (DRL) and other learning-based methods have brought new opportunities for dealing with these problems. However, most learning-based methods depend on the simulation environment. How to transfer the algorithms trained in the simulation environment to the physical system has become the key to solving the problem.
</p>

<p>
  The goal of the competition is to find good sim2real methods. In the competition task, the robot needs to find 5 targets whose positions are randomly generated in the maze, and complete the activation in a specified order. In 3 minutes, the agent which finds 5 targets and activates them at the fastest and safe speed wins. The competition will be divided into two stages: in the first stage, the agent only needs to complete the above tasks in the simulation environment. We will select the agents with higher scores to participate in the second stage. In the second stage, we deploy the participating agents to the physical robot and test them in a real environment. At this stage, the agent needs to deal with the difference between the simulated robot actuator and the physical actuator, and the state difference caused by a different environment. The agent can adjust the model according to the feedback data and results.  
</p>
<p><a href="https://github.com/liushasha2021/EvalAI-CoG/blob/challenge/templates/a.gif">Demo Vidio</a></p>
<p><a href="https://github.com/DRL-CASIA/COG-sim2real-challenge">Github Baseline Algrithom</a></p>
<p><a href="https://github.com/DRL-CASIA/COG-sim2real-challenge/blob/main/2022%20CoG%20RoboMaster%20Sim2Real%20Challenge%20Rules-EN-v1.4.pdf">CoG Challenge Rules(English Version)</a></p>
<p><a href="https://github.com/DRL-CASIA/COG-sim2real-challenge/blob/main/2022%20CoG%20RoboMaster%20Sim2Real%20Challenge%20Rules-CN-v1.4.pdf">CoG Challenge Rules(中文版)</a></p>

<h5>Track</h5>
<h6>Track 1: complete information-based track.</h6>
<p>The observations provided by the environment include the position and the angle of the EP robot, the position and the angle of the defensive robot, the positions of five goals, image, lidar and some other necessary information.  The algorithm is required to output the speed (x-direction, y-direction, yaw) and fire command  to control the robot.</p>

<h6>Track 2: image-based track</h6>
<p>The robot can't obtain the position and the angle of the EP robot, but other information can be obtained from the environment.. The algorithm is required to output the speed (x-direction, y-direction, yaw) and fire command  to control the robot.</p>
